---
layout: post
title: Scrapy 命令行工具(2) #标题
tagline: 命令行工具
category: python      #分类
author: wali    #作者
tag: Scrapy     #标签
ghurl:        #github url
ghurl_zip:   #github zip下载
comments: true

post_nav: ["1.配置设置", "2.默认的scrapy项目结构", "3.工具命令"] 
group_tag: Scrapy 1.7
---

Scrapy是通过`scrapy` 命令行工具进行控制的。 这里我们称之为 `Scrapy tool`以用来和子命令进行区分。 对于子命令，我们称为 "command" 或者 "Scrapy commands"。


# 1.配置设置

Scrapy将会在以下路径中寻找记录了配置参数的 scrapy.cfg 文件, 该文件以ini的方式记录:

- 1.`/etc/scrapy.cfg` 或 `c:\scrapy\scrapy.cfg` (系统层面)
- 2.`~/.config/scrapy.cfg ($XDG_CONFIG_HOME)` 及`~/.scrapy.cfg ($HOME)` 作为全局(用户层面)设置
- 3.在scrapy项目根路径下的 `scrapy.cfg`


从这些文件中读取到的设置按照以下顺序合并：项目中`scrapy.cfg` > `~/.scrapy.cfg` > `/etc/scrapy.cfg`

# 2.默认的scrapy项目结构

```txt
/
|--scrapy.cfg
|--myproject/
     |--__init__.py
     |--items.py
     |--pipelines.py
     |--settings.py
     |--spiders/
         |--__init__.py
         |--spider1.py
         |--spider2.py
```

`scrapy.cfg 存放的目录被认为是 项目的根目录` 。该文件中包含python模块名的字段定义了项目的设置。例如:

```txt
[settings]
default = myproject.settings
```

# 3.工具命令

`scrapy`提供了可用的内置命令的列表。您可用通过运行命令来获取每个命令的详细内容。

```txt
# 单个命令详情
scrapy <command> -h

#查看所有可用的命令
scrapy -h
```

Scrapy提供了两种类型的命令。一种必须在Scrapy项目中运行(针对项目(`Project-specific`)的命令)，另外一种则不需要(`全局命令`)。全局命令在项目中运行时的表现可能会与在非项目中运行有些许差别(因为可能会使用项目的设定)。

#### 全局命令

命令|描述|语法|是否需要项目|例子|
-|-|-|-|-|
startproject|在文件夹下创建一个名为`project_name`的scrapy项目|`scrapy startproject <project_name>`|否|scrapy startproject myproject|
settings|在项目中运行时，该命令将会输出项目的设定值，否则输出Scrapy默认设定|`scrapy settings [options]`|否|scrapy settings --get BOT_NAME <br> scrapybot <br> scrapy settings --get DOWNLOAD_DELAY <br> 0|
runspider|在未创建项目的情况下，运行一个编写在Python文件中的spider|`scrapy runspider <spider_file.py>`|否|scrapy runspider myspider.py|
shell|启动Scrapy终端|`scrapy shell [url]`|否|scrapy shell http://www.example.com/some/page.html|
fetch|使用Scrapy下载器(downloader)下载给定的URL，并将获取到的内容送到标准输出|`scrapy fetch <url>`|否|scrapy fetch --nolog http://www.example.com/some/page.html|
view|在浏览器中打开给定的URL，并以Scrapy spider获取到的形式展现。 有些时候spider获取到的页面和普通用户看到的并不相同。 因此该命令可以用来检查spider所获取到的页面，并确认这是您所期望的|`scrapy view <url>`|否|scrapy view http://www.example.com/some/page.html|
version|输出Scrapy版本。配合`-v`运行时,该命令同时输出Python,Twisted 以及平台的信息，方便bug提交|`scrapy version [-v]`|否||


#### 项目(project)命令

命令|描述|语法|是否需要项目|例子|
-|-|-|-|-|
crawl|使用spider进行爬取|`scrapy crawl <spider>`|是|scrapy crawl myspider|
check|运行contract检查|`scrapy check [-l] <spider>`|是|scrapy check -l <br> scrapy check|
list|列出当前项目中所有可用的spider。每行输出一个spider|`scrapy list`|是|scrapy list|
edit|使用 EDITOR 中设定的编辑器编辑给定的spider|`scrapy edit <spider>`|是|scrapy edit spider1|
parse|获取给定的URL并使用相应的spider分析处理。如果您提供 `--callback` 选项，则使用spider的该方法处理，否则使用 `parse`|`scrapy parse <url> [options]`|是|scrapy parse http://www.example.com/ -c parse_item|
genspider|在当前项目中创建spider|`scrapy genspider [-t template] <name> <domain>`|是|scrapy genspider -l <br> scrapy genspider -d basic|
bench|运行benchmark测试|`scrapy bench`|是||


### parse选项

选项|描述|
-|-|
`--spider=SPIDER`|跳过自动检测spider并强制使用特定的spider|
`--a NAME=VALUE`|设置spider的参数(可能被重复|
`--callback` or `-c`|spider中用于解析返回(response)的回调函数|
`--pipelines`|在pipeline中处理item|
`--rules` or `-r`|使用 CrawlSpider 规则来发现用来解析返回(response)的回调函数|
`--noitems`|不显示爬取到的item|
`--nolinks`|不显示提取到的链接|
`--nocolour`|避免使用pygments对输出着色|
`--depth` or `-d`|指定跟进链接请求的层次数|
`--verbose` or `-v`|显示每个请求的详细信息|

























