---
layout: post
title: Scrapy 安装并创建运行爬虫项目(1) #标题
tagline: 安装并创建运行爬虫项目 
category: python      #分类
author: wali    #作者
tag: Scrapy     #标签
ghurl:        #github url
ghurl_zip:   #github zip下载
comments: true

post_nav: ["1.安装Scrapy", "2.创建Scrapy项目", "3.运行爬虫"] 
group_tag: Scrapy 1.7
---

Scrapy是一个用于爬取网站并提取结构化数据的应用程序框架，可用于各种有用的应用程序，例如数据挖掘，信息处理或历史档案。

- [Scrapy 官方手册](https://docs.scrapy.org/en/latest/index.html "https://docs.scrapy.org/en/latest/index.html")
- [Scrapy 中文指南](https://doc.yonyoucloud.com/doc/wiki/project/scrapy/items.html "https://doc.yonyoucloud.com/doc/wiki/project/scrapy/items.html")

小菜之前有过一篇[anaconda安装](/python/2019/05/13/anaconda.html "/python/2019/05/13/anaconda.html"),爬虫项目小菜就使用`anaconda`来管理包。`anaconda`管理包也很方便。如果小伙伴们不想安装`anaconda`，那小菜建议使用`pipenv`来管理下载python包。具体教程[Flask 准备工作](/python/2019/07/08/flask-1.html  "/python/2019/07/08/flask-1.html")

好了，大家掌握了一门包管理工具后，下面就正式开始了。

# 1.安装Scrapy

创建一个文件夹`testScrapy`，进入`testScrapy`，启动cmd

```
#进入虚拟环境
activate PY37

#安装Scrapy
conda install -c conda-forge scrapy

#查看Scrapy是否安装
pip list

#找到Scrapy说明安装成功
Scrapy             1.7.3
```


# 2.创建Scrapy项目

在开始抓取之前，您将必须设置一个新的Scrapy项目。输入要存储代码并运行的目录：

```txt
scrapy startproject tutorial
```
如果运行命令出现错误，请参考[scrapy 解决方案](/python/2019/08/12/scrapy-error-1.html "/python/2019/08/12/scrapy-error-1.html")。如果在后面遇到相同问题，请小伙们自行解决。


这将创建一个`tutorial`包含以下内容的目录：

```txt
tutorial
|--scrapy.cfg            # 配置文件
|--tutorial              # 项目
    |--__init__.py
    |--items.py          # project items definition file
    |--middlewares.py    # project middlewares file
    |--pipelines.py      # project pipelines file
    |--settings.py       # project settings file
    |--spiders           # 我们的爬虫写在这个文件夹下
        |--__init__.py
```


#### 手写一个爬虫
创建一个`quotes_spider.py`文件，将其放到`spiders`文件夹下

```diff
tutorial
|--scrapy.cfg            
|--tutorial              
    |--__init__.py
    |--items.py          
    |--middlewares.py   
    |--pipelines.py      
    |--settings.py       
    |--spiders           
        |--__init__.py
+       |--quotes_spider.py
```

#### quotes_spider.py

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)
```

# 3.运行爬虫

为了使我们的蜘蛛工作，请转到项目的顶级目录并运行：`tutorial`

```
scrapy crawl quotes 
```

该命令使用`quotes`我们刚刚添加的名称运行Spider ，它将发送对该quotes.toscrape.com域的一些请求。您将获得类似于以下的输出：

```txt
... (omitted for brevity)
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened
2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished)
...
```

现在，检查当前目录中的文件。您应该注意，已经创建了两个新文件：`quotes-1.html` 和 `quotes-2.html`，按照我们的parse方法说明，其内容分别为URL。

到目前为止，我们已经编写了一个爬虫，并且成功的运行了起来






