---
layout: post
title: Scrapy Feed exports(7) #标题
tagline: Feed exports
category: python      #分类
author: wali    #作者
tag: Scrapy     #标签
ghurl:        #github url
ghurl_zip:   #github zip下载
comments: true

post_nav: ["1.序列化方式","2.存储","3.存储URI参数","4.本地文件系统","5.Settings"] 
group_tag: Scrapy 
---

实现爬虫时最经常提到的需求就是能合适的保存爬取到的数据，或者说，生成一个带有爬取数据的`导出文件`(通常叫做`export feed`)，来供其他系统使用。

Scrapy 自带了 `Feed` 输出，并且支持多种序列化格式(`serialization format`)及存储方式(`storage backends`)

# 1.序列化方式

#### Serialization formats

`feed`输出使用到了 `Item exporters`。其自带支持的类型有:
- JSON
- JSON lines
- CSV
- XML

您也可以通过 `FEED_EXPORTERS` 设置扩展支持的属性。

#### JSON

JSON:
- FEED_FORMAT: json
- 使用的 exporter: JsonItemExporter
- 大数据量情况下使用 JSON 请参见 这个警告

#### JSON lines

JSON lines:
- FEED_FORMAT: jsonlines
- 使用的 exporter: JsonLinesItemExporter

#### CSV

CSV:
- FEED_FORMAT: csv
- 使用的 exporter: CsvItemExporter

#### XML

XML:
- FEED_FORMAT: xml
- 使用的exporter: XmlItemExporter

#### Pickle

Pickle:
- FEED_FORMAT: pickle
- 使用的 exporter: PickleItemExporter

#### Marshal

Marshal:
- FEED_FORMAT: marshal
- 使用的exporter: MarshalItemExporter


# 2.存储

使用 feed 输出时您可以通过使用 URI(通过 `FEED_URI` 设置) 来定义存储端。feed 输出支持 URI 方式支持的多种存储后端类型。

自带支持的存储后端有:
- 本地文件系统
- FTP
- S3 (需要 boto)
- 标准输出

有些存储后端会因所需的外部库未安装而不可用。例如，S3只有在`boto`库安装的情况下才可使用。


# 3.存储URI参数

存储 `URI` 也包含参数。当`feed`被创建时这些参数可以被覆盖
- `%(time)s` - 当 feed 被创建时被 timestamp 覆盖
- `%(name)s` - 被 spider 的名字覆盖

其他命名的参数会被 `spider`同名的属性所覆盖。例如， 当 feed 被创建时， `%(site_id)s `将会被 `spider.site_id` 属性所覆盖。

下面用一些例子来说明:

1.存储在`FTP`，每个 `spider`一个目录:

```
ftp://user:password@ftp.example.com/scraping/feeds/%(name)s/%(time)s.json
```

2.存储在`S3`，每一个spider一个目录:

```
s3://mybucket/scraping/feeds/%(name)s/%(time)s.json
```

# 4.本地文件系统

将 feed 存储在本地系统:
- URI scheme: `file`
- URI 样例: `file:///tmp/export.csv`
- 需要的外部依赖库:`none`

`注意`: (只有)存储在本地文件系统时，您可以指定一个绝对路径 `tmp/export.csv` 并忽略协议(scheme)。不过这仅仅只能在 Unix 系统中工作。

#### FTP

将feed存储在FTP服务器
- URI scheme:`ftp`
- URI 样例:`ftp://user:pass@ftp.example.com/path/to/export.csv`
- 需要的外部依赖库:`none`

#### S3

将feed存储在 Amazon S3 
- URI scheme: `s3`
- URI 样例: 
-   s3://mybucket/path/to/export.csv
-   s3://mybucket/path/to/export.csv
- 需要的外部依赖库: `boto`

您可以通过在 `URI` 中传递 `user/pass` 来完成 AWS 认证，或者也可以通过下列的设置来完成:

`AWS_ACCESS_KEY_ID` `AWS_SECRET_ACCESS_KEY`

#### 标准输出

feed 输出到 `Scrapy`进程的标准输出
- URI scheme: `stdout`
- URI 样例: `stdout:`
- 需要的外部依赖库: none

# 5.Settings

这些是配置 feed 输出的设定:
- FEED_URI (必须)
- FEED_FORMAT
- FEED_STORAGES
- FEED_STORAGE_FTP_ACTIVE
- FEED_STORAGE_S3_ACL
- FEED_EXPORTERS
- FEED_STORE_EMPTY
- FEED_EXPORT_ENCODING
- FEED_EXPORT_FIELDS
- FEED_EXPORT_INDENT

设置|默认值|描述|
-|-|-|
FEED_URI|None|输出 feed 的 URI|
FEED_FORMAT||输出 feed 的序列化格式|
FEED_EXPORT_ENCODING|None|使用feed编码|
FEED_EXPORT_FIELDS|None|使用FEED_EXPORT_FIELDS选项定义要导出的字段及其顺序|
FEED_EXPORT_INDENT|0||
FEED_STORE_EMPTY|False|是否导出空的feeds（即没有item)|
FEED_STORAGES|{}|包含您的项目支持的其他提要存储后端的字典。密钥是URI方案，值是存储类的路径。|
FEED_STORAGE_FTP_ACTIVE|False||
FEED_STORAGE_S3_ACL|''||
FEED_STORAGES_BASE|{<br>'': 'scrapy.extensions.feedexport.FileFeedStorage',<br>'file': 'scrapy.extensions.feedexport.FileFeedStorage',<br>'stdout': 'scrapy.extensions.feedexport.StdoutFeedStorage',<br>'s3': 'scrapy.extensions.feedexport.S3FeedStorage',<br> 'ftp': 'scrapy.extensions.feedexport.FTPFeedStorage',<br>}||
FEED_EXPORTERS|{}||
FEED_EXPORTERS_BASE|{<br>'json': 'scrapy.exporters.JsonItemExporter',<br>'jsonlines': 'scrapy.exporters.JsonLinesItemExporter',<br> 'jl': 'scrapy.exporters.JsonLinesItemExporter',<br>'csv': 'scrapy.exporters.CsvItemExporter',<br>'xml': 'scrapy.exporters.XmlItemExporter',<br> 'marshal': 'scrapy.exporters.MarshalItemExporter',<br> 'pickle': 'scrapy.exporters.PickleItemExporter',<br>}||










